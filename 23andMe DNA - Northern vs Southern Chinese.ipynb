{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23andME SNP logistic regression :\n",
    "I trained a logistic regression model on data from the [Human Genome Diversity Project](http://www.hagsc.org/hgdp/). The goal for this project was to figure out whether my DNA is a closer to match to northern or southern Han chinese DNA.\n",
    "\n",
    "Future Improvements: Since I was running the algorithm from my computer without any big data/distributed computing paradigms I had to take a random samples of SNPs from my DNA and the genome database. When I get better with a big data computing paradigm I'd like to rerun the algorithm and see if there are any significant changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping and cleaning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Eric = pd.read_csv(\"Eric_23andMe.txt\", delimiter= \"\\t\", usecols=[\"rsid\", \"genotype\"]).set_index(\"rsid\").transpose()\n",
    "sample= pd.read_csv(\"HGDP_SampleList.txt\",header =None, names = [\"code\"])\n",
    "key = pd.read_csv('key.csv', header= None, delimiter = \" \", usecols=[1,2],names = [\"code\", \"group\"])\n",
    "sample = sample.merge(key, how=\"left\", on=\"code\")\n",
    "han = [\"Han\", \"Han.NChina\"]\n",
    "hans = sample[sample['group'].apply(lambda x: x in han)]\n",
    "han_codes = [\"SNP\"]+list(hans.code.unique())\n",
    "final = pd.read_table(\"HGDP_FinalReport_Forward.txt\",delimiter = \"\\t\", usecols=han_codes, index_col=0,\\\n",
    "                      dtype=\"category\").transpose()\n",
    "final = pd.merge(key,final, how=\"right\", right_index=True, left_on=\"code\")\n",
    "final = final.set_index(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericlu/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "SNPs = list(set(final.columns)&set(Eric.columns))\n",
    "toy_SNPs = [\"group\"]+list(np.random.choice(SNPs, 1000))\n",
    "Eric_toy = Eric.loc[:,toy_SNPs]\n",
    "final.index\n",
    "final_toy = final.loc[:,toy_SNPs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2899) (44, 2899)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 0.14259291,  0.16774511,  0.1822772 ]),\n",
       " 'score_time': array([ 0.00047278,  0.00046277,  0.0004859 ]),\n",
       " 'test_score': array([ 0.75      ,  0.78571429,  0.78571429]),\n",
       " 'train_score': array([ 0.78571429,  0.76666667,  0.76666667])}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "encoded_toy = pd.get_dummies(final_toy).drop(\"group_Han.NChina\", axis = 1)\n",
    "y = encoded_toy['group_Han']\n",
    "x = encoded_toy.drop('group_Han', axis =1)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0, random_state = 0)\n",
    "\n",
    "Eric_encoded = pd.get_dummies(Eric_toy.drop('group', axis =1))\n",
    "\n",
    "for col in set(x.columns)-set(Eric_encoded.columns):\n",
    "    Eric_encoded[col] = 0\n",
    "for col in set(Eric_encoded.columns)-set(x.columns):\n",
    "    x[col] = 0\n",
    "print(Eric_encoded.shape, x.shape)\n",
    "alg = linear_model.LogisticRegressionCV()\n",
    "alg.fit(x_train, y_train)\n",
    "\n",
    "cv_results = model_selection.cross_validate(alg, x, y, return_train_score=True)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model on my DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 2899) (1, 2899)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.22741986,  0.77258014]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape, Eric_encoded.shape)\n",
    "alg.fit(x,y)\n",
    "alg.predict_proba(Eric_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: based on the model that I trained, my DNA is a closer match to southern Han Chinese dataset. This result seems reasonable since 2 of my grandparents are southern chinese, 1 is northern chinese, and 1 is on the border."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
